{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parent_directory_path():\n",
    "    return os.path.dirname(\n",
    "        os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "    )\n",
    "\n",
    "def add_directory_to_sys_path(directory_path):\n",
    "    return sys.path.insert(-1, directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tqdm import *\n",
    "import telegram_send\n",
    "import os, sys, inspect\n",
    "add_directory_to_sys_path(get_parent_directory_path())\n",
    "\n",
    "from dataloader import DataLoader\n",
    "import lstm as lstm_utils\n",
    "import validate as validator\n",
    "from constant import Constant\n",
    "from parameters import Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## utilities.py\n",
    "\n",
    "def get_numbers_of_batch(dataloader):\n",
    "    return dataloader.num_batch()\n",
    "\n",
    "\n",
    "def load_dataloader(METADATA_PATH, BATCH_SIZE, LABEL_COLUMN_NAME, EXTRACTED_FEATURE_NAME=None, is_training=False):\n",
    "    return DataLoader(file_path=METADATA_PATH\n",
    "                                  , batch_size=BATCH_SIZE\n",
    "                                  , label_column_name=LABEL_COLUMN_NAME\n",
    "                                  , is_training=is_training\n",
    "                                  , use_extracted_feature=EXTRACTED_FEATURE_NAME\n",
    "                                  )\n",
    "\n",
    "def get_batch(train_dataloader):\n",
    "    return train_dataloader.next_batch()\n",
    "\n",
    "\n",
    "def chunk_batch_X(batch_X, chunk_size, timestep):\n",
    "    return list(\n",
    "        map(\n",
    "            lambda X: X[:, timestep*chunk_size:(timestep+1)*chunk_size]\n",
    "            , batch_X\n",
    "        )\n",
    "    )\n",
    "\n",
    "def split_X(batch_X, split_indices):\n",
    "    \"\"\"\n",
    "    params @batch_X [a list of ndarray] (length : batch_size)\n",
    "    params @split_indices [a list of integer]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def debug():\n",
    "        def test(X):\n",
    "            return list(map(lambda X: X.shape,  np.hsplit(X, split_indices)))\n",
    "        print(list(map(lambda X: test(X), batch_X)))\n",
    "    # debug()\n",
    "    \n",
    "    return list(map(lambda X: np.hsplit(X, split_indices), batch_X))\n",
    "\n",
    "def get_split_indices(IMAGE_WIDTH, WINDOW_NUM):\n",
    "    return list(map(lambda i: i*int(IMAGE_WIDTH/WINDOW_NUM), range(1, WINDOW_NUM+1)))\n",
    "\n",
    "def get_current_chunk(batch_X_chunks, selected_window_index):\n",
    "    return list(\n",
    "        map(\n",
    "            lambda X_chunks: X_chunks[selected_window_index]\n",
    "            , batch_X_chunks\n",
    "        )\n",
    "   )\n",
    "\n",
    "def get_context_chunk(batch_X_chunks, selected_window_index, context_window_size=1):\n",
    "    \"\"\"\n",
    "    params @batch_X_chunks[a list of a list of ndarrays]\n",
    "    params @selected_window\n",
    "    \"\"\"\n",
    "    def _index_clamp_to_positive(index): \n",
    "        return 0 if index < 0 else index\n",
    "    \n",
    "    def _get_before_chunk_indices():\n",
    "        return _index_clamp_to_positive(selected_window_index-context_window_size), selected_window_index\n",
    "    \n",
    "    def _get_after_chunk_indices():\n",
    "        return selected_window_index+1, selected_window_index+1+context_window_size\n",
    "        \n",
    "    return list(\n",
    "        map(\n",
    "            lambda X_chunks: X_chunks[_get_before_chunk_indices()[0]:_get_before_chunk_indices()[1]]\\\n",
    "                              + X_chunks[_get_after_chunk_indices()[0]:_get_after_chunk_indices()[1]]\n",
    "            , batch_X_chunks\n",
    "        )\n",
    "   )\n",
    "\n",
    "def initialize_batch_votings(BATCH_SIZE, N_CLASS):\n",
    "    return np.zeros([BATCH_SIZE, N_CLASS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### models.py\n",
    "\n",
    "def set_X(IMAGE_HEIGHT, window_size):\n",
    "    return tf.placeholder(tf.float32, [None, IMAGE_HEIGHT, window_size])\n",
    "\n",
    "def set_window_size(IMAGE_WIDTH, WINDOW_NUM):\n",
    "    return int(IMAGE_WIDTH/WINDOW_NUM)\n",
    "\n",
    "def set_y(N_CLASS):\n",
    "    return tf.placeholder(tf.float32, [None, N_CLASS])\n",
    "\n",
    "def unstack_X_by_timestep(X, window_size):\n",
    "    return tf.unstack(X, num=window_size, axis=2)\n",
    "\n",
    "def set_rnn_cell(RNN_TYPE, EMBEDDING_DIMENSION):\n",
    "    return RNN_TYPE(EMBEDDING_DIMENSION, state_is_tuple=False )\n",
    "\n",
    "def wrap_attention(rnn_cell, attention_length):\n",
    "    return tf.contrib.rnn.AttentionCellWrapper(\n",
    "        rnn_cell, attention_length\n",
    "        , state_is_tuple=False\n",
    "    )\n",
    "\n",
    "def stack_rnn_cells(make_rnn_cell, num_layers):\n",
    "    return list(map(\n",
    "        lambda i: make_rnn_cell\n",
    "        , range(num_layers)\n",
    "    ))\n",
    "\n",
    "def layer_rnn_cells(stacked_rnn_cells):\n",
    "    return tf.nn.rnn_cell.MultiRNNCell(\n",
    "        stacked_rnn_cells\n",
    "        , state_is_tuple=False\n",
    "    )\n",
    "\n",
    "def run_rnn(rnn_cells, X):\n",
    "    return tf.contrib.rnn.static_rnn(\n",
    "        rnn_cells, X, dtype=tf.float32\n",
    "    )\n",
    "\n",
    "def set_fully_connected_layer(X, num_hidden_units, activation):\n",
    "    return tf.layers.dense(X, num_hidden_units, activation=activation)\n",
    "\n",
    "def get_logits(X, N_CLASS):\n",
    "    return tf.layers.dense(X, N_CLASS)\n",
    "\n",
    "def calculate_losses(LOSS_FUNCTION, labels, logits):\n",
    "    return LOSS_FUNCTION(labels, logits)\n",
    "\n",
    "def calculate_mean_loss(losses):\n",
    "    return tf.reduce_mean(losses)\n",
    "\n",
    "def set_train_step(Optimizer, target_loss):\n",
    "    return Optimizer.minimize(target_loss)\n",
    "\n",
    "def build_attention_model(project_constant, data_constant, experiment_parameters):\n",
    "    X = set_X(\n",
    "        data_constant.IMAGE_HEIGHT\n",
    "          , set_window_size(\n",
    "              data_constant.IMAGE_WIDTH\n",
    "              , experiment_parameters.WINDOW_NUM\n",
    "          )\n",
    "    )\n",
    "    \n",
    "    y = set_y(experiment_parameters.N_CLASS)\n",
    "        \n",
    "    # unstack X\n",
    "    unstacked_X = unstack_X_by_timestep(\n",
    "        X\n",
    "        , set_window_size(\n",
    "          data_constant.IMAGE_WIDTH\n",
    "          , experiment_parameters.WINDOW_NUM\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # stack rnn cells\n",
    "    stacked_rnn_cells = []\n",
    "    for i in range(experiment_parameters.NUM_RNN_LAYERS):\n",
    "        stacked_rnn_cells.append(\n",
    "          wrap_attention(\n",
    "                set_rnn_cell(\n",
    "                    experiment_parameters.RNN_TYPE\n",
    "                    , experiment_parameters.EMBEDDING_DIMENSION\n",
    "                )\n",
    "                , experiment_parameters.ATTENTION_LENGTH\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # run rnn\n",
    "    _, encoding = run_rnn(\n",
    "        layer_rnn_cells(stacked_rnn_cells)\n",
    "        , unstacked_X\n",
    "    )\n",
    "   \n",
    "    # classification\n",
    "    logits = get_logits(\n",
    "        set_fully_connected_layer(\n",
    "            encoding\n",
    "            , experiment_parameters.NUM_FC_HIDDEN_UNITS\n",
    "            , experiment_parameters.Activation\n",
    "        )\n",
    "        , experiment_parameters.N_CLASS\n",
    "    )\n",
    "    \n",
    "    # loss\n",
    "    mean_loss = calculate_mean_loss(\n",
    "        calculate_losses(experiment_parameters.LOSS_FUNCTION\n",
    "                         , labels=y, logits=logits)\n",
    "    )\n",
    "    train_step = set_train_step(experiment_parameters.Optimizer, mean_loss)\n",
    "    \n",
    "    # accuracy\n",
    "    prediction_indices = tf.argmax(logits, 1)\n",
    "    #accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    \n",
    "    return X, y, logits, mean_loss, train_step, prediction_indices\n",
    "\n",
    "def get_batch_predictions(batch_votings, labels):\n",
    "    return np.equal(np.argmax(batch_votings, 1), np.argmax(labels, 1))\n",
    "\n",
    "def add_votings(batch_votings, prediction_indices):\n",
    "    batch_votings[np.arange(prediction_indices.shape[0]), prediction_indices] += 1\n",
    "    return batch_votings\n",
    "\n",
    "def calculate_accuracy(batch_predictions):\n",
    "    return np.mean(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5863\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x2b4586fd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x1b22adcf8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x11fcffcf8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.contrib.rnn.python.ops.rnn_cell.AttentionCellWrapper object at 0x28cd20390>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2454c11fba49af987e43c5a2364742"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]\n",
      " [ 0.  0.  2.]]\n"
     ]
    }
   ],
   "source": [
    "### MAIN.py\n",
    "def main(project_constant, data_constant, experiment_parameters):\n",
    "    try:\n",
    "        train_dataloader = load_dataloader(project_constant.METADATA_PATH\n",
    "                        , experiment_parameters.BATCH_SIZE\n",
    "                        , project_constant.LABEL_COLUMN_NAME\n",
    "                        , data_constant.FEATURE_NAME\n",
    "                        , is_training=True\n",
    "                       )\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            X, y, logits, mean_loss, train_step, prediction_indices = build_attention_model(\n",
    "                project_constant\n",
    "                , data_constant\n",
    "                , experiment_parameters\n",
    "            )\n",
    "                        \n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                #for epoch in range(EPOCH_NUM):\n",
    "                for epoch in range(1):\n",
    "\n",
    "                    #for i in range(get_numbers_of_batch(train_dataloader)):\n",
    "                    for batch_index in range(1):\n",
    "                        batch_X, batch_y = get_batch(train_dataloader)\n",
    "                        batch_votings = initialize_batch_votings(\n",
    "                            experiment_parameters.BATCH_SIZE\n",
    "                            , experiment_parameters.N_CLASS\n",
    "                        )\n",
    "                        batch_X_chunks = split_X(batch_X\n",
    "                                                 , get_split_indices(\n",
    "                                                     data_constant.IMAGE_WIDTH\n",
    "                                                     , experiment_parameters.WINDOW_NUM\n",
    "                                                 )\n",
    "                                                )\n",
    "                        #for selected_window_index in tnrange(experiment_parameters.WINDOW_NUM):\n",
    "                        for selected_window_index in tnrange(2):\n",
    "                            current_chunk = get_current_chunk(batch_X_chunks, selected_window_index)\n",
    "                            feed_dict = {X: current_chunk, y: batch_y}\n",
    "                            mean_loss_, prediction_indices_ = sess.run([mean_loss, prediction_indices], feed_dict)\n",
    "                            batch_votings = add_votings(batch_votings, prediction_indices_)\n",
    "\n",
    "                        print(batch_votings)\n",
    "                        batch_predictions = get_batch_predictions(batch_votings, labels=batch_y)\n",
    "                        accuracy = calculate_accuracy(\n",
    "                            get_batch_predictions(batch_votings, labels=batch_y)\n",
    "                        )\n",
    "                        \n",
    "                        telegram_send.send(messages=[str(batch_index)\n",
    "                                                    , str(batch_predictions)\n",
    "                                                    , str(accuracy)]\n",
    "                                          )\n",
    "                        \n",
    "\n",
    "                            #context_chunk = get_context_chunk(batch_X_chunks, selected_window_index\n",
    "                            #                                  , context_window_size=1)\n",
    "    except Exception as e:\n",
    "        telegram_send.send(messages=[str(e)])\n",
    "\n",
    "    return None\n",
    "\n",
    "main(Constant.Project2, Constant.Data.ChromaStftHop512, Parameters.Attention.Experiment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playground.py\n",
    "project_constant = Constant.Project2\n",
    "data_constant = Constant.Data.ChromaStftHop512\n",
    "experiment_parameters = Parameters.Attention.Experiment1\n",
    "\n",
    "def telegram_try_except_test():\n",
    "    try:\n",
    "        raise ValueError(\"Some Errors!\")\n",
    "    except Exception as e:\n",
    "        aaa = np.zeros([2, 3])\n",
    "        bbb = 1.239\n",
    "        ccc = [1, 2, 3]\n",
    "        telegram_send.send(messages=[str(aaa), str(bbb), str(ccc)])\n",
    "        \n",
    "telegram_try_except_test()\n",
    "\n",
    "def set_np_array():\n",
    "    BATCH_SIZE = experiment_parameters.BATCH_SIZE\n",
    "    N_CLASS = experiment_parameters.N_CLASS\n",
    "    aaa = np.zeros([BATCH_SIZE, N_CLASS])\n",
    "    bbb = np.random.rand(BATCH_SIZE, N_CLASS)\n",
    "    \n",
    "    aaa += bbb\n",
    "    chunk = np.zeros([BATCH_SIZE, N_CLASS])\n",
    "    max_indices = np.argmax(aaa, 1)\n",
    "    print(max_indices)\n",
    "    print(max_indices.shape[0])\n",
    "    chunk[np.arange(max_indices.shape[0]), max_indices] += 1\n",
    "    print(chunk)\n",
    "    \n",
    "    train_dataloader = load_dataloader(project_constant.METADATA_PATH\n",
    "                    , experiment_parameters.BATCH_SIZE\n",
    "                    , project_constant.LABEL_COLUMN_NAME\n",
    "                    , data_constant.FEATURE_NAME\n",
    "                    , is_training=True\n",
    "                   )\n",
    "    _, batch_y = get_batch(train_dataloader)\n",
    "    \n",
    "    predictions = np.equal(np.argmax(aaa, 1), np.argmax(batch_y, 1))\n",
    "    accuracy = np.mean(predictions)\n",
    "    #print(accuracy)\n",
    "    \n",
    "#set_np_array()\n",
    "\n",
    "def test(project_constant, data_constant, experiment_parameters):\n",
    "    train_dataloader = load_dataloader(project_constant.METADATA_PATH\n",
    "                    , experiment_parameters.BATCH_SIZE\n",
    "                    , project_constant.LABEL_COLUMN_NAME\n",
    "                    , data_constant.FEATURE_NAME\n",
    "                    , is_training=True\n",
    "                   )\n",
    "    _, batch_y = get_batch(train_dataloader)\n",
    "    print(batch_y)\n",
    "    \n",
    "    return None\n",
    "\n",
    "#test(Constant.Project2, Constant.Data.ChromaStftHop512, Parameters.Attention.Experiment1)\n",
    "\n",
    "\"\"\"\n",
    "def small_test(experiment_parameters):\n",
    "    return list(\n",
    "        map(\n",
    "        lambda i: wrap_attention(\n",
    "            set_rnn_cell(\n",
    "                experiment_parameters.RNN_TYPE\n",
    "                , experiment_parameters.NUM_HIDDENS\n",
    "            )\n",
    "            , experiment_parameters.ATTENTION_LENGTH\n",
    "        )\n",
    "        , [1, 2]\n",
    "    )\n",
    "               )\n",
    "\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
